{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0eeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import CAPS\n",
    "from CAPS import ProgressBar\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from tpot import TPOTRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import os.path\n",
    "from random import sample, seed\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "data_path = [\"..\", \"data\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49b3f4",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7f9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped = pd.DataFrame(columns=['Why', 'How Many'])\n",
    "\n",
    "user_input = input(\"Skip creating master_frame1.csv?(Y/n) \")\n",
    "# Since creating master_frame1 can take a not insignificant amount of time\n",
    "# this line is included so that user may skip doing this.\n",
    "if user_input != 'Y':\n",
    "\n",
    "    # Cleaning of Appointments data frame\n",
    "    apts_df = pd.read_csv(os.path.join(*data_path, \"Appointments.csv\"), low_memory=False)\n",
    "\n",
    "    rows = apts_df.shape[0]\n",
    "    dropped.loc[dropped.shape[0], :] = ('Rows in Appointments.csv', rows)  # 311168\n",
    "\n",
    "    # Fix ClientIDS - data is useless if we are missing them\n",
    "    apts_df = apts_df.replace('#NULL!', np.nan)\n",
    "    apts_df.dropna(subset=[\"ClientID\"], inplace=True)\n",
    "    apts_df[\"ClientID\"] = apts_df[\"ClientID\"].astype(float).astype(int).astype(str)\n",
    "\n",
    "    dropped.loc[dropped.shape[0], :] = ('NA in ClientID', apts_df.shape[0] - rows)\n",
    "\n",
    "    # Hide away 20% of Clients for final validation\n",
    "    seed(34)\n",
    "    hide = sample(sorted(apts_df['ClientID'].unique()), int(len(apts_df['ClientID'].unique()) * .2))\n",
    "    apts_df = apts_df[~apts_df['ClientID'].isin(hide)]\n",
    "    pd.Series(hide).to_csv(os.path.join(*data_path, 'ClientsHiddenAway'), index=False)\n",
    "\n",
    "    rows = apts_df.shape[0]\n",
    "    dropped.loc[dropped.shape[0], :] = ('Rows after hiding 20% of clients', rows)\n",
    "\n",
    "    # Make Dates a Datetime object\n",
    "    apts_df['Date'] = pd.to_datetime(apts_df['Date'])\n",
    "\n",
    "    apts_df = apts_df[\n",
    "        (apts_df[\"AttendanceDescription\"] == 'Attended') | (\n",
    "            apts_df['AttendanceDescription'] == 'Client No Show')\n",
    "    ]\n",
    "    dropped.loc[dropped.shape[0], :] = ('Neither \"Attended\" nor \"Client No Show\"', apts_df.shape[0] - rows)\n",
    "    rows = apts_df.shape[0]\n",
    "\n",
    "    # Drop some appointment types that Dr. Erekson said were not of interest\n",
    "    apts_df = apts_df[~apts_df[\"AppType\"].isin(['Bio Individual',\n",
    "                                                'Bio Intake',\n",
    "                                                'Medication Re-check',\n",
    "                                                'Medication Initial',\n",
    "                                                'Mindfulness Meditation',\n",
    "                                                'Bio Class Assignment',\n",
    "                                                'Phone',\n",
    "                                                'Testing',\n",
    "                                                'Biofeedback Training',\n",
    "                                                'Email',\n",
    "                                                'Biofeedback Class Assignments',\n",
    "                                                'Biofeedback Research',\n",
    "                                                'Group Note',\n",
    "                                                'Supervision'\n",
    "                                                ])\n",
    "                      ]\n",
    "\n",
    "    dropped.loc[dropped.shape[0], :] = ('Remove certain appointment types', apts_df.shape[0] - rows)\n",
    "\n",
    "    # Make TherapistID as string\n",
    "    apts_df[\"TherapistID\"] = apts_df[\"TherapistID\"].astype(str)\n",
    "    # print(apts_df[\"TherapistID\"].unique())  # No null values\n",
    "\n",
    "    # ### Aggregate Sessions\n",
    "\n",
    "    pat_df = pd.read_csv(os.path.join(*data_path, \"PatientInformation.csv\"), low_memory=False)\n",
    "    client_IDs = pat_df.loc[~pat_df[\"ClientID\"].isin(hide), 'ClientID'].unique().astype(str)\n",
    "\n",
    "    def attend(x):\n",
    "        x = list(x)\n",
    "        if x[-1] == 'Attended':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    n = len(client_IDs)\n",
    "\n",
    "    master_df = pd.DataFrame(\n",
    "        columns=['PatientID', 'TherapistID', 'StartDate', 'EndDate',\n",
    "                 'NumOfAttended', 'Crisis', 'LastAppShowed', 'AttendRate', 'OneAppIsIntake']\n",
    "    )  # Empty data-frame\n",
    "\n",
    "    print('Aggregating Sessions')\n",
    "    for j, client in enumerate(client_IDs):\n",
    "        ProgressBar(j, n)\n",
    "\n",
    "        # Data frame containing the AppIDs TherapistIDs, and Dates for a particular client\n",
    "        app = apts_df[\n",
    "            apts_df['ClientID'] == client\n",
    "        ][['AppID', 'TherapistID', 'Date', 'AppType', 'AttendanceDescription']]\n",
    "\n",
    "        # Sorts the data frame based on date\n",
    "        app.sort_values('Date', inplace=True)\n",
    "\n",
    "        if app.shape[0] < 1:\n",
    "            # If the data-frame has no appointments, it is not included\n",
    "            continue\n",
    "\n",
    "        # Create column 'Elapsed' which has the number of days since the last attended appointment\n",
    "        # of the same therapist for attended appointments and days since last appointment of the\n",
    "        # same therapist for no-show appointments\n",
    "        app['Elapsed'] = (\n",
    "            app.groupby(['TherapistID', 'AttendanceDescription'])['Date'].diff()\n",
    "        ).apply(lambda x: x.days)\n",
    "\n",
    "        app.loc[app['AttendanceDescription'] == 'Client No Show', 'Elapsed'] = app.groupby(['TherapistID'])[\n",
    "            'Date'].diff().apply(lambda x: x.days)\n",
    "\n",
    "        app.fillna(0, inplace=True)\n",
    "\n",
    "        # If the appointment of type crisis, yes(1)/no(0)\n",
    "        app['Crisis'] = app['AppType'].apply(lambda x: 1 if x.lower()[-6:] == 'crisis' else 0)\n",
    "\n",
    "        app['Session'] = 0  # Assigns zeros to, while also creating, a column called Session\n",
    "        sess_num = 0\n",
    "\n",
    "        # Number sessions\n",
    "        for therapist in app['TherapistID'].unique():\n",
    "            sessions = []\n",
    "            # Within a therapist if the time between appointments exceeds 180 days, new session begins\n",
    "            for i in range(app.loc[app['TherapistID'] == therapist, :].shape[0] - 1):\n",
    "                sessions.append(sess_num)\n",
    "                if app.loc[app['TherapistID'] == therapist, :].iloc[i + 1]['Elapsed'] > 180:\n",
    "                    sess_num += 1\n",
    "            sessions.append(sess_num)\n",
    "            app.loc[app['TherapistID'] == therapist, 'Session'] += sessions\n",
    "            sess_num += 1\n",
    "\n",
    "        # Makes a data-frame with columns for the start and end dates\n",
    "        temp = pd.DataFrame(list(\n",
    "            app.groupby('Session')['Date'].apply(\n",
    "                lambda x: [min(x), max(x)])\n",
    "        ), columns=['StartDate', 'EndDate'])\n",
    "\n",
    "        temp.insert(loc=0, column='PatientID', value=client)\n",
    "        temp.insert(loc=1, column='TherapistID', value=app.groupby('Session')[\"TherapistID\"].apply(lambda x: x.iloc[0])\n",
    "                    )  # Adds the therapist ID that corresponds to the therapist during the session dates\n",
    "        temp['NumOfAttended'] = (\n",
    "            app[app['AttendanceDescription'] == 'Attended'].groupby(\n",
    "                'Session')[\"AppID\"].apply(lambda x: len(x))\n",
    "        )  # Adds number of appointments in session\n",
    "        temp['Crisis'] = (\n",
    "            app.groupby('Session')[\"Crisis\"].apply(lambda x: max(x))\n",
    "        )  # 1 if a crisis appointment type is part of the session, 0 if not\n",
    "        temp['LastAppShowed'] = (\n",
    "            app.groupby('Session')['AttendanceDescription'].apply(attend)\n",
    "        )  # 1 if the client showed up to the last appointment in that session, 0 if not\n",
    "        temp['AttendRate'] = (\n",
    "            app.groupby('Session')['AttendanceDescription'].apply(\n",
    "                lambda x: np.mean(x == 'Attended'))\n",
    "        )\n",
    "\n",
    "        # Some client meet a particular therapist once for intake but not again\n",
    "        # This OneAppIsIntake column is so that we can drop them later\n",
    "        temp['OneAppIsIntake'] = app.groupby('Session')['AppType'].apply(\n",
    "            lambda x: 1 if (len(x) == 1) and (list(x)[0][-6:].lower() == 'intake') else 0\n",
    "        )\n",
    "\n",
    "        master_df = pd.concat([master_df, temp])\n",
    "\n",
    "    master_df['NumOfAttended'].replace({np.nan: 0}, inplace=True)\n",
    "    master_df['StartDate'] = pd.to_datetime(master_df['StartDate'])\n",
    "    master_df['EndDate'] = pd.to_datetime(master_df['EndDate'])\n",
    "    rows = master_df.shape[0]\n",
    "    dropped.loc[dropped.shape[0], :] = ('Number of Sessions', rows)\n",
    "    master_df = master_df[master_df['OneAppIsIntake'] != 1].drop(\n",
    "        'OneAppIsIntake', axis=1)\n",
    "    dropped.loc[dropped.shape[0], :] = ('Drop Sessions that are solely Intake', master_df.shape[0] - rows)\n",
    "    master_df.to_csv(os.path.join(*data_path, 'master_frame1.csv'), index=False)\n",
    "    print('master_frame1.csv created')\n",
    "\n",
    "    master_df.index = range(master_df.shape[0])\n",
    "\n",
    "# End: if user_input != 'Y':\n",
    "\n",
    "# ### Match patient information to session\n",
    "print('Matching patient information to session')\n",
    "\n",
    "pat_df = pd.read_csv(os.path.join(*data_path, \"PatientInformation.csv\"), low_memory=False)\n",
    "pat_df['ClientID'] = pat_df['ClientID'].astype(float).astype(int).astype(str)\n",
    "pat_df['notedate'] = pd.to_datetime(pat_df['notedate'])\n",
    "key = pd.read_csv(os.path.join(*data_path, 'Key.csv'))\n",
    "\n",
    "# Rename columns so that they are not codes but descriptions\n",
    "col = list(pat_df.columns)\n",
    "for j, i in enumerate(col):\n",
    "    i = i.upper()\n",
    "    if i in list(key['Column Name']):\n",
    "        col[j] = str(key[key['Column Name'] == i].Description.values[0])\n",
    "pat_df.columns = col\n",
    "\n",
    "\n",
    "# Drop columns marked as OBSOLETE\n",
    "drop = [i for i in pat_df.columns if 'OBSOLETE' in i]\n",
    "pat_df.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "# Oddly, the non-inactive Gender is all NA values\n",
    "pat_df['Gender'] = pat_df['Gender <inactive>']\n",
    "\n",
    "# Drop columns marked as inactive\n",
    "drop = [i for i in pat_df.columns if 'inactive' in i]\n",
    "pat_df.drop(drop, axis=1, inplace=True)\n",
    "\n",
    "# Subset to columns that Dr. Davey Erekson suggested\n",
    "data_dict = pd.read_csv(os.path.join(*data_path, 'DataDictionary.csv'))\n",
    "\n",
    "keep = ['ClientID', 'notedate']\n",
    "keep.extend(data_dict.loc[data_dict['DrEreksonRecommended'].isin(\n",
    "    ['yes', 'maybe']), 'PatientInfoColumn'].values)\n",
    "pat_df = pat_df[keep]\n",
    "\n",
    "\n",
    "# Replace <No Response> with Nan for uniformity\n",
    "pat_df.replace('<No Response>', np.nan, inplace=True)\n",
    "\n",
    "# NA in age\n",
    "pat_df['age'] = pd.to_numeric(pat_df['age'], errors='coerce')\n",
    "# Tyler Mansfield's cleaning said CAPS does not have clients under 16\n",
    "pat_df.loc[pat_df['age'] <= 16, 'age'] = np.nan\n",
    "\n",
    "# Drop the rows that lack the client-answered part of the information\n",
    "subset_pat = pat_df.copy().loc[\n",
    "    ~pat_df.loc[:, 'Confusion about religious beliefs or values':].isna().apply(lambda x: all(x), axis=1), :\n",
    "]\n",
    "# Some rows have a non-NA while the rest of the answers were not filled out,\n",
    "# but this might be caused by how the information was encoded, not because\n",
    "# the client answered only those questions\n",
    "# So that is why it looks at the columns after 'Confusion about religious beliefs or values'\n",
    "\n",
    "master_df = pd.read_csv(os.path.join(*data_path, 'master_frame1.csv'))\n",
    "# Convert columns to certian types\n",
    "master_df['PatientID'] = master_df['PatientID'].astype(str)\n",
    "master_df['StartDate'] = pd.to_datetime(master_df['StartDate'])\n",
    "master_df['EndDate'] = pd.to_datetime(master_df['EndDate'])\n",
    "subset_pat['ClientID'] = subset_pat['ClientID'].astype(str)\n",
    "subset_pat['notedate'] = pd.to_datetime(subset_pat['notedate'])\n",
    "\n",
    "# Add columns to master_df that are in pat_df, except for ClientID\n",
    "add_columns = pd.DataFrame(columns=subset_pat.drop('ClientID', axis=1).columns)\n",
    "master_df = master_df.join(add_columns)\n",
    "\n",
    "\n",
    "stable = ['Gender', 'Race / Ethnicity', 'International Student',\n",
    "          \"Have you been diagnosed with an autism-spectrum disorder or Asperger's Syndrome?\",\n",
    "          'First Generation', 'Financial Stress Past', 'Religion', 'Religion Importance']\n",
    "\n",
    "last_time = [c for c in pat_df.columns if 'Last time' in c]\n",
    "\n",
    "changing = list(set(pat_df.loc[:, 'Gender':].columns) - set(stable) - set('age') - set(last_time))\n",
    "\n",
    "convert_dictionary = {'Within the last year': 365, 'More than 5 years ago': 'same',\n",
    "                      'Within the last 1-5 years': 'same', 'Within the last 2 weeks': 14,\n",
    "                      'Within the last month': 30, 'Never': 'same'}\n",
    "\n",
    "\n",
    "def last_time_convert(value, time_past):\n",
    "    x = convert_dictionary[value] + time_past\n",
    "    if x <= 14:\n",
    "        return 'Within the last 2 weeks'\n",
    "    if x <= 30:\n",
    "        return 'Within the last month'\n",
    "    if x <= 365:\n",
    "        return 'Within the last year'\n",
    "    if x <= 1825:\n",
    "        return 'Within the last 1-5 years'\n",
    "    return 'More than 5 years ago'\n",
    "\n",
    "lost = []\n",
    "session_survey = {'master_frame1 row': ['ClientID', 'StartDate - MatchedSurvey', 'dict(StartDate, notedate)']}\n",
    "session_survey_extra = []\n",
    "\n",
    "for i in range(master_df.shape[0]):\n",
    "    ProgressBar(i, master_df.shape[0])\n",
    "    # Get a session (row) of master_df\n",
    "    client = master_df.loc[i]\n",
    "    # Find the patient information for that client\n",
    "    information = subset_pat[subset_pat['ClientID'] == client['PatientID']].sort_values('notedate')\n",
    "\n",
    "    if information.shape[0] == 0:\n",
    "        lost.append(['NoPaper', client['PatientID']])\n",
    "        session_survey_extra.append([client['PatientID'], np.nan, np.nan, np.nan, np.nan])\n",
    "        # If there is not information, then add to lost list so that you may look at these clients\n",
    "        continue\n",
    "\n",
    "    # Calculate how difference between session start and survey notedate \n",
    "    # and make it the information index\n",
    "    information.index = (client['StartDate'] - information['notedate']\n",
    "                         ).apply(lambda x: x.days)  # if notedate is after StartDate, returns a negative value\n",
    "\n",
    "    information.sort_index()  # low to high\n",
    "\n",
    "    # Take the nearst survey to assign for patient information of that session in master_df\n",
    "    try:\n",
    "        survey = information.loc[\n",
    "            (information.index >= 0) & (information.index <= 90), :].iloc[0]  # First tries to find a survey 90 days before\n",
    "        # the session started\n",
    "    except IndexError:\n",
    "        survey = information.iloc[np.argmin(abs(information.index)), :]\n",
    "        # if try fails, then use closest (absolute) survey\n",
    "\n",
    "    # add the matched survey index and all the information indexs so that we may look at them\n",
    "    session_survey[i] = [client['PatientID'], survey.name, {'StartDate':client['StartDate'], 'notedate': information['notedate'].values}]\n",
    "\n",
    "    good_match = (survey.name >= 0) & (survey.name <= 180)\n",
    "    session_survey_extra.append([client['PatientID'],\n",
    "                                 client['TherapistID'],\n",
    "                                 client['StartDate'],\n",
    "                                 survey['notedate'],\n",
    "                                 good_match])\n",
    "    # end of ss_extra\n",
    "\n",
    "    # Fill missing values using other surveys\n",
    "    if information.shape[0] > 1:\n",
    "        for c in stable:\n",
    "            if np.isnan(survey.isna()[c]):\n",
    "                for i in information.drop(survey.index).index:\n",
    "                    if ~np.isnan(information.isna().loc[i, c]):\n",
    "                        survey[c] = information.loc[i, c].values\n",
    "                        break\n",
    "\n",
    "        # For changing, look only at the surveys that were within\n",
    "        # 90 days before the session start\n",
    "        for c in changing:\n",
    "            if np.isnan(survey.isna()[c]):\n",
    "                for i in information.drop(survey.index).index:\n",
    "                    if (~np.isnan(information.isna().loc[i, c])\n",
    "                        ) & (information.loc[i].index <= 90\n",
    "                             ) & (information.loc[i].index >= 0):\n",
    "                        survey[c] = information.loc[i, c].values\n",
    "                        break\n",
    "\n",
    "        for c in last_time:\n",
    "            if np.isnan(survey.isna()[c]):\n",
    "                for i in information.drop(survey.index).index:\n",
    "                    if (~np.isnan(information.isna().loc[i, c])) & (information.loc[i].index >= 0):\n",
    "                        value = information.loc[i, c].values\n",
    "                        time_past = information.loc[i].index\n",
    "                        fill = value\n",
    "                        if convert_dictionary[value] != 'same':\n",
    "                            # Later, answers that are beyond one year will be changed to a 0,\n",
    "                            # so not it does not need to be changed if it already beyond a year\n",
    "                            fill = last_time_convert(value, time_past)\n",
    "\n",
    "                        survey[c] = fill\n",
    "                        break\n",
    "\n",
    "        if np.isnan(survey.isna()['age']):\n",
    "            for i in information.drop(survey.index).index:\n",
    "                if (~np.isnan(information.isna().loc[i, c])\n",
    "                    ) & (information.loc[i].index <= 90\n",
    "                         ) & (information.loc[i].index >= 0):\n",
    "                    age = information.loc[i, c].values\n",
    "                    fill_age = age + ((survey.index - information.loc[i].index) / 365.25)\n",
    "                    survey['age'] = fill_age\n",
    "                    break\n",
    "\n",
    "    master_df.loc[i, subset_pat.drop('ClientID', axis=1).columns] = survey\n",
    "\n",
    "# End of session-survey matching\n",
    "\n",
    "rows = master_df.shape[0]\n",
    "master_df.dropna(subset='notedate', inplace=True)  # Drop sessions that had no surveys\n",
    "dropped.loc[dropped.shape[0], :] = ('No surveys', rows - master_df.shape[0])\n",
    "\n",
    "master_df.index = range(master_df.shape[0])\n",
    "\n",
    "master_df['notedate'] = pd.to_datetime(master_df['notedate'])\n",
    "\n",
    "# New column DateDifference that is the difference between the session start date and the notedate\n",
    "date_diff = master_df[['StartDate', 'notedate']].apply(\n",
    "    lambda x: (x['StartDate'] - x['notedate']).days, axis=1)\n",
    "master_df.insert(loc=master_df.columns.get_loc('notedate') + 1,\n",
    "                 column='DateDifference', value=date_diff)\n",
    "\n",
    "# Update age\n",
    "master_df['age'] = master_df['age'].astype(float) + (master_df['DateDifference'] / 365)\n",
    "\n",
    "# ### To Numeric\n",
    "\n",
    "# Answers on a scale are converted to numbers\n",
    "scale1 = {'Never': 0, '1 time': 1, '2-3 times': 2, '4-5 times': 3,\n",
    "          'More than 5 times': 4}\n",
    "scale2 = {'Never stressful': 0, 'Rarely stressful': 1, 'Sometimes stressful': 2,\n",
    "          'Often stressful': 3, 'Always stressful': 4}\n",
    "scale3 = {'0': 0, 'Rarely (0-1 nights)': 0, 'Sometimes (2-3 nights)': 1, 'Often (4+ nights)': 2}\n",
    "scale4 = {'Strongly disagree': -2, 'Somewhat disagree': -1,\n",
    "          'Neutral': 0, 'Somewhat agree': 1, 'Strongly agree': 2}\n",
    "scale5 = {'Very unimportant': -2, 'Unimportant': -1,\n",
    "          'Neutral': 0, 'Important': 1, 'Very important': 2}\n",
    "scale_list = [scale1, scale2, scale3, scale4, scale5]\n",
    "\n",
    "# Make replace dictionary\n",
    "replace = {}\n",
    "for c in master_df.columns:\n",
    "    unique_v = master_df[c].dropna().unique()\n",
    "    for scale in scale_list:\n",
    "        if set(scale.keys()).issuperset(unique_v):\n",
    "            replace[c] = scale\n",
    "            break\n",
    "\n",
    "master_df.replace(replace, inplace=True)\n",
    "\n",
    "\n",
    "last_time = [c for c in master_df.columns if 'Last time' in c]\n",
    "for c in last_time:\n",
    "    master_df[c].replace({'Within the last 2 weeks': 1, 'Within the last month': 1,\n",
    "                          'Within the last year': 1, 'Within the last 1-5 years': 0,\n",
    "                          'More than 5 years ago': 0, 'Never': 0, '<No Response>': 0}, inplace=True)\n",
    "# 1 if more recent than a year\n",
    "# Seth set this somewhat arbitrarily, so it may be good\n",
    "# to ask Dr. Erekson if the year threshold is the most relevent of the options to clinicians\n",
    "\n",
    "rename_1 = {}\n",
    "for c in last_time:\n",
    "    master_df[c] = master_df[c].astype(float)\n",
    "    rename_1[c] = c.replace('Last time', 'Recent')\n",
    "\n",
    "    # Add to ModifiedName\n",
    "    data_dict.loc[data_dict['PatientInfoColumn'] == c,\n",
    "                  'ModifiedName'] = c.replace('Last time', 'Recent')\n",
    "    # End of adding\n",
    "\n",
    "master_df.rename(columns=rename_1, inplace=True)\n",
    "\n",
    "academic_columns = pd.get_dummies(master_df['Academic Status'], prefix='', prefix_sep='')\n",
    "academic_other = ['Non-student', 'Other (please specify)', 'Non-degree student',\n",
    "                  'Faculty or staff', 'High-school student taking college classes']\n",
    "\n",
    "for c in academic_columns.drop(academic_other, axis=1).columns:\n",
    "    master_df.insert(loc=master_df.columns.get_loc(\n",
    "        'Academic Status'), column=c, value=academic_columns[c])\n",
    "master_df.drop('Academic Status', axis=1, inplace=True)\n",
    "\n",
    "modified_names = list(academic_columns.drop(academic_other, axis=1).columns)\n",
    "\n",
    "# Add to ModifiedName\n",
    "for i in range(len(modified_names)):\n",
    "    index = data_dict.loc[data_dict['PatientInfoColumn'] == 'Academic Status'].index[0] + i\n",
    "    if data_dict.at[index, 'ModifiedName'] != modified_names[i]:\n",
    "        data_dict.loc[index, 'ModifiedName'] = modified_names[i]\n",
    "# End of adding\n",
    "\n",
    "\n",
    "# Convert certain columns to yes(1)/no(1)\n",
    "\n",
    "for c in ['Prior Counseling', 'Prior Meds']:\n",
    "    master_df[c] = master_df[c].apply(lambda x: 0 if x == 'Never' else np.nan if pd.isna(x) else 1)\n",
    "\n",
    "rename_2 = {'Gender': 'Female',\n",
    "            'Relationship Status': 'MarriedMale',\n",
    "            'Race / Ethnicity': 'RacialMinority',\n",
    "            'Sexual Orientation': 'SexOrientationMinority',\n",
    "            'Religion': 'ReligiousMinority',\n",
    "            'In what college is your current major?': 'NursingOrLaw',\n",
    "            'Housing Other': 'Homeless'}\n",
    "\n",
    "master_df.rename(columns=rename_2, inplace=True)\n",
    "\n",
    "# Add to ModifiedName\n",
    "for c in rename_2.keys():\n",
    "    data_dict.loc[data_dict['PatientInfoColumn'] == c,\n",
    "                  'ModifiedName'] = rename_2[c]\n",
    "# End of adding\n",
    "\n",
    "master_df['Female'] = master_df['Female'].apply(\n",
    "    lambda x: 1 if x == 'Female' else 0 if x == 'Male' else np.nan)\n",
    "\n",
    "master_df['MarriedMale'] = (master_df[['Female', 'MarriedMale']]\n",
    "                            ).apply(lambda x: 1 if x[0] == 0 and x[1] == 'Married' else 0, axis=1)\n",
    "\n",
    "master_df['RacialMinority'] = (master_df['RacialMinority']\n",
    "                               ).apply(lambda x: 0 if x == 'White' else\n",
    "                                       np.nan if pd.isna(x) else 1)\n",
    "\n",
    "master_df['SexOrientationMinority'] = (master_df['SexOrientationMinority']\n",
    "                                       ).apply(lambda x: 0 if x == 'Heterosexual / Straight' or x == 0 else np.nan if pd.isna(x) else 1)\n",
    "agn_or_ath = 'AgnosticOrAtheist'\n",
    "master_df.insert(loc=master_df.columns.get_loc('ReligiousMinority'),\n",
    "                 column=agn_or_ath,\n",
    "                 value=(\n",
    "    master_df['ReligiousMinority'].apply(\n",
    "        lambda x: 1 if x == 'Agnostic' or x == 'Atheist' else np.nan if pd.isna(x) else 0)\n",
    "))\n",
    "\n",
    "# Add to ModifiedName\n",
    "index = data_dict.loc[data_dict['PatientInfoColumn'] == 'Religion'].index[0] + 1\n",
    "if data_dict.at[index, 'ModifiedName'] != agn_or_ath:\n",
    "    data_dict.loc[index, 'ModifiedName'] = agn_or_ath\n",
    "# End of adding\n",
    "\n",
    "data_dict.to_csv(os.path.join(*data_path, 'DataDictionary.csv'), index=False)\n",
    "\n",
    "master_df['ReligiousMinority'] = master_df['ReligiousMinority'].apply(lambda x: 0 if (\n",
    "    x == 'Christian' or x == 'Atheist' or x == 'Agnostic') else np.nan if pd.isna(x) else 0)\n",
    "\n",
    "# Dr. Erekson said 'In what college is you major' might be relevent is the client is\n",
    "# a nursing or law student\n",
    "master_df['NursingOrLaw'] = 0  # most client's did not answer this question,\n",
    "# of those who did, none were nursing or law\n",
    "# Here is a guess of what the code might look like if there were:\n",
    "# master['NursingOrLaw'] = master['NursingOrLaw'].apply(lambda x: 1 if x=='Law' or x=='Nursing' else 0)\n",
    "\n",
    "\n",
    "def to_homeless(x):\n",
    "    x = str(x)\n",
    "    if 'homeless' in x.lower() or 'none' in x.lower() or x.lower() == 'no':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "master_df['Homeless'] = master_df['Homeless'].apply(to_homeless)\n",
    "# This is a hard one to convert, since it is not multiple choice\n",
    "# After taking a look as some of the answers, looking for homeless and none,\n",
    "# somewhere in the string or if the string is simply 'no' seemed like a wise choice\n",
    "\n",
    "\n",
    "master_df.to_csv(os.path.join(*data_path, 'master_frame2.csv'), index=False)\n",
    "print('master_frame2.csv created')\n",
    "\n",
    "master_df = pd.read_csv(os.path.join(*data_path, 'master_frame2.csv'))\n",
    "\n",
    "print('Columns that are not numeric:')\n",
    "for c in master_df.columns:\n",
    "    if master_df.dtypes[c] not in ['int64', 'float64', 'uint8']:\n",
    "        print('{0:30}\\t{1}'.format(c, master_df.dtypes[c]))\n",
    "# PatientID and TherapistID should be treated as objects\n",
    "\n",
    "print(dropped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f58e3e",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7450b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_df = pd.read_csv(os.path.join(*data_path, 'master_frame2.csv'))\n",
    "\n",
    "# If column has NAs, print how many\n",
    "print('{0:30}\\t{1}'.format('Column', 'Number of NAs'))\n",
    "for i, j in zip(master_df.columns, master_df.isna().apply(sum, axis=0)):\n",
    "    if j != 0:\n",
    "        print('{0:30}\\t{1}'.format(i, j))\n",
    "\n",
    "master_df['PatientID'] = master_df['PatientID'].astype(str)\n",
    "master_df['TherapistID'] = master_df['TherapistID'].astype(str)\n",
    "\n",
    "# Fill some NAs\n",
    "\n",
    "master_df['age'].fillna(master_df['age'].median(), inplace=True)\n",
    "\n",
    "\n",
    "# For the following columns, there are a sizeable number of NAs, which will be filled with 0\n",
    "# The assumption is that when these clients skip the question, they answer in the negative\n",
    "no_answer_zero = ['Sexual Orientation']\n",
    "no_answer_zero += [c for c in master_df.columns if 'Recent' in c]\n",
    "no_answer_zero += ['In the past week, about how many nights has it taken you more than half an hour to fall asleep?',\n",
    "                   'In the past week, about how many nights have you woken during the night AND needed more than half an hour to fall back to sleep?',\n",
    "                   'In the past two weeks, have you taken a substance to help with sleep? Please consider prescription medication, over the counter medication and supplements, and substances such as alcohol and marijuana.',\n",
    "                   'In what college is your current major?',\n",
    "                   'Academics',\n",
    "                   'Social life/relationships',\n",
    "                   'Emotional well-being',\n",
    "                   'Pornography',\n",
    "                   'ROTC',\n",
    "                   'Military Stress']\n",
    "# Pornography, ROTC, and Military Stress had few than 20,000 NAs in master_df but more than 1,000\n",
    "# The rest had more than 20,000 NAs\n",
    "\n",
    "# Make dictionary for replacing values\n",
    "replace = {}\n",
    "for c in no_answer_zero:\n",
    "    replace[c] = 0\n",
    "\n",
    "master_df.fillna(replace, inplace=True)\n",
    "\n",
    "median_fill = dict(master_df.loc[:, 'Female':].dropna().apply(np.median, axis=0))\n",
    "temp = master_df.loc[:, 'Female':].fillna(median_fill)\n",
    "Matrix = temp.to_numpy()\n",
    "print('Calculating SVD')\n",
    "U, s, VT = np.linalg.svd(Matrix)\n",
    "\n",
    "\n",
    "n = 10  # A somewhat arbitrary choice\n",
    "replace = pd.DataFrame(U[:, :n] @ np.diag(s[:n]) @ VT[:n, :],\n",
    "                       columns=master_df.loc[:, 'Female':].columns)\n",
    "\n",
    "\n",
    "def to_answer(x, answers):\n",
    "    '''Rounds x to the closes number in the list set_values'''\n",
    "    answers = np.array(answers)\n",
    "    index = np.argmin(np.abs(answers - x))\n",
    "    return answers[index]\n",
    "\n",
    "\n",
    "# Use the replace data frame to fill in NAs\n",
    "for c in replace.columns:\n",
    "    a = {'answers': master_df[c].dropna().unique()}\n",
    "    master_df.loc[master_df[c].isna(), c] = (replace.loc[master_df[c].isna(), c]\n",
    "                                             ).apply(to_answer, **a)\n",
    "\n",
    "\n",
    "# When talking with Dr. Davey Erekson, it was suggested that we aggregate\n",
    "# the follow up questions about disabilities and trauma to be a number count of the yes's\n",
    "\n",
    "# Disability\n",
    "# rename column name that ask about disabilities\n",
    "rename = {'Are you registered, with the office for disability services on this campus, as having a documented and diagnosed disability?':\n",
    "          'Disabilities'}\n",
    "master_df.rename(columns=rename, inplace=True)\n",
    "\n",
    "# Add to ModifiedName\n",
    "data_dict = pd.read_csv(os.path.join(*data_path, 'DataDictionary.csv'))\n",
    "data_dict.loc[data_dict['PatientInfoColumn'] == list(rename.keys())[0],\n",
    "              'ModifiedName'] = rename[list(rename.keys())[0]]\n",
    "# End of adding\n",
    "\n",
    "disability = [c for c in master_df.columns if 'If you selected, \"Yes\"' in c]\n",
    "disability_values = master_df[disability].apply(lambda x: sum(\n",
    "    x.dropna()), axis=1)  # adds up the number of disabilities\n",
    "disability_values[\n",
    "    (pd.to_numeric(master_df['Disabilities'], errors='coerce').replace(\n",
    "        {np.nan: 0}) > 0) & (disability_values == 0)\n",
    "] += 1  # This is included for those who said they had a disability but did not mark the follow up questions\n",
    "master_df.loc[:, 'Disabilities'] = disability_values\n",
    "# Assigns number of disabilities to Disabilities column\n",
    "\n",
    "master_df.drop(disability, axis=1, inplace=True)\n",
    "\n",
    "# Trauma\n",
    "# column names that ask about childhood trauma\n",
    "modified_names = ['ChildhoodTrauma', 'Trauma']\n",
    "childhood = [c for c in master_df.columns if 'Childhood' in c]\n",
    "childhood_values = master_df[childhood].apply(lambda x: 1 if sum(x.dropna()) > 0 else 0, axis=1)\n",
    "master_df.insert(loc=master_df.columns.get_loc(\n",
    "    childhood[0]), column=modified_names[0], value=childhood_values)\n",
    "\n",
    "\n",
    "# ChildhoodTrauma column is a yes/no\n",
    "\n",
    "trauma = [c for c in master_df.columns if (\n",
    "    'Please select the traumatic event' in c) & ('Childhood' not in c)]\n",
    "# Column names that are about trauma but not childhood trauma\n",
    "trauma_values = master_df[trauma].apply(lambda x: sum(x.dropna()), axis=1)\n",
    "master_df.insert(loc=master_df.columns.get_loc(\n",
    "    trauma[0]), column=modified_names[1], value=trauma_values)\n",
    "master_df.loc[:, 'Trauma'] += master_df[childhood].apply(lambda x: sum(x.dropna()), axis=1).values\n",
    "# Trauma is the number of yes's in the trauma columns, including childhood\n",
    "\n",
    "# Add to ModifiedName\n",
    "for i, c in enumerate([childhood[0], trauma[0]]):\n",
    "    index = data_dict.loc[data_dict['PatientInfoColumn'] == c].index[0]\n",
    "    if data_dict.at[index, 'ModifiedName'] != modified_names[i]:\n",
    "        data_dict.loc[index, 'ModifiedName'] = modified_names[i]\n",
    "\n",
    "data_dict.to_csv(os.path.join(*data_path, 'DataDictionary.csv'), index=False)\n",
    "# End of adding\n",
    "\n",
    "master_df.drop(childhood, axis=1, inplace=True)\n",
    "master_df.drop(trauma, axis=1, inplace=True)\n",
    "\n",
    "master_df.to_csv(os.path.join(*data_path, 'master_frame3.csv'), index=False)\n",
    "\n",
    "print('master_frame3.csv created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769557b1",
   "metadata": {},
   "source": [
    "### Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeba411",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Upload data and drop nonsense data\n",
    "scores = pd.read_csv(os.path.join(*data_path, \"OQ.csv\"), low_memory=False)\n",
    "scores.replace('#VALUE!', np.nan, inplace=True)\n",
    "scores[\"CurrentScore\"].replace([180, -3000, -1982, -1978, -1967,\n",
    "                               -1956, -1900, -914, 0], np.nan, inplace=True)\n",
    "scores.dropna(subset=[\"ClientID\", \"CurrentScore\"], inplace=True)\n",
    "master_df = pd.read_csv(os.path.join(*data_path, 'master_frame3.csv'))\n",
    "\n",
    "master_df['PatientID'] = master_df['PatientID'].astype(int).astype(str)\n",
    "master_df['StartDate'] = pd.to_datetime(master_df['StartDate'])\n",
    "master_df['EndDate'] = pd.to_datetime(master_df['EndDate'])\n",
    "\n",
    "# Convert OQ times to datetime objects\n",
    "scores.index = pd.to_datetime(scores[\"AdministrationDate\"])\n",
    "\n",
    "\n",
    "# Add columns to master data frame\n",
    "master_df[\n",
    "    ['IncomingSubClinical', 'RawScore', 'NonLogged',\n",
    "        'SubClinicalScore', 'NetDrop', 'ClinicallySignificantChange',\n",
    "        'SignificantChangeToSubClinical', 'SignificantDeterioration',\n",
    "        'PenultOrUltOQ', 'FirstOQ', 'NumberOfOQs']\n",
    "] = 0\n",
    "# IncomingSubClincial is a feature of the patient\n",
    "\n",
    "print('Calculating scores')\n",
    "for i in master_df.index:\n",
    "    ProgressBar(i, master_df.index[-1])\n",
    "\n",
    "    # Pull up client\n",
    "    client = master_df.loc[i]\n",
    "\n",
    "    # Gather relevant OQ surveys\n",
    "    mask = scores[\"ClientID\"] == client[\"PatientID\"]\n",
    "    OQ_scores = scores[mask]\n",
    "    Scores = OQ_scores.sort_index().loc[\n",
    "        str(client['StartDate']):str(client['EndDate'])\n",
    "    ].groupby(level=0)['CurrentScore'].mean()\n",
    "\n",
    "    # If two of the first four scores are in the \"normal\" range,\n",
    "    # the session is marked with 1 in the columns IncomingSubClinical\n",
    "    if np.sum(Scores[:4] < 64) >= 2:\n",
    "        master_df.loc[i, 'IncomingSubClinical'] = 1\n",
    "\n",
    "    if len(Scores) < 2:\n",
    "        master_df.loc[i, ['RawScore', 'NonLogged', 'SubClinicalScore',\n",
    "                          'NetDrop', 'ClinicallySignificantChange',\n",
    "                          'SignificantChangeToSubClinical',\n",
    "                          'SignificantDeterioration', 'PenultOrUltOQ',\n",
    "                          'FirstOQ', 'NumberOfOQs']] = (\n",
    "            np.nan, np.nan, np.nan,\n",
    "            np.nan, np.nan,\n",
    "            np.nan,\n",
    "            np.nan, Scores.max() if len(Scores) > 0 else np.nan,\n",
    "            Scores.max() if len(Scores) > 0 else np.nan, len(Scores))\n",
    "        continue\n",
    "\n",
    "    # Regression\n",
    "    # We use a modified scale for our x-axis, a mesh between time\n",
    "    # and appointment frequency\n",
    "    y_vals = Scores.to_numpy()\n",
    "    y_log = np.log(y_vals)\n",
    "    x_vals = np.array(list(map(lambda x: (x - Scores.index[0]).days, Scores.index)))\n",
    "\n",
    "    # Calculate slopes (Exercise 6.1) in Volume 3\n",
    "    x_bar = np.mean(x_vals)\n",
    "    x_log = np.log(1 + x_vals)\n",
    "    x_logbar = np.mean(x_log)\n",
    "    y_bar = np.mean(y_vals)\n",
    "    ylog_bar = np.mean(y_log)\n",
    "    m = np.sum(x_log * y_log - x_log * ylog_bar) / np.sum(x_log * x_log - x_log * x_logbar)\n",
    "\n",
    "    m_NoLog = np.sum(x_vals * y_vals - x_vals * y_bar) / np.sum(x_vals * x_vals - x_vals * x_bar)\n",
    "\n",
    "    # Other scores\n",
    "    drop = y_vals[0] - y_vals[-1]\n",
    "    first_score = y_vals[0]\n",
    "    sig_change = int(drop >= 14)\n",
    "    sig_to_sub = int(y_vals[0] > 64 and y_vals[-1] <= 63 and drop >= 14)\n",
    "    sig_deter = int(drop <= -14)\n",
    "    sub = int(np.min(y_vals) <= 63)\n",
    "    penult_or_ult = np.max(y_vals[-2:])\n",
    "    master_df.loc[i, ['RawScore', 'NonLogged', 'SubClinicalScore',\n",
    "                      'NetDrop', 'ClinicallySignificantChange',\n",
    "                      'SignificantChangeToSubClinical',\n",
    "                      'SignificantDeterioration', 'PenultOrUltOQ',\n",
    "                      'FirstOQ', 'NumberOfOQs']] = (\n",
    "        m * -1, m_NoLog * -1, sub,\n",
    "        drop, sig_change,\n",
    "        sig_to_sub,\n",
    "        sig_deter, penult_or_ult,\n",
    "        first_score, len(y_vals))\n",
    "\n",
    "\n",
    "master_df.to_csv(os.path.join(*data_path, 'master_frame4.csv'), index=False)\n",
    "\n",
    "print('master_frame4.csv created')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ee40b",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963a4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "master_df = pd.read_csv(os.path.join(*data_path, 'master_frame4.csv'))\n",
    "rows = master_df.shape[0]\n",
    "master_df = master_df.loc[(master_df['DateDifference'] < 180) & (master_df['DateDifference'] > 0), :]\n",
    "master_df.to_csv(os.path.join(*data_path, 'master_frame5.csv'), index=False)\n",
    "\n",
    "print('Survey not within 180 days before', master_df.shape[0] - rows)\n",
    "print(f\"Number of clients in test/train data: {master_df['PatientID'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff5c7b",
   "metadata": {},
   "source": [
    "### Some demographic info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed349096",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df = pd.read_csv(os.path.join(*data_path, 'master_frame5.csv'))\n",
    "dg = master_df.drop_duplicates(subset=['PatientID'], keep='first')\n",
    "ages = [int(i) for i in np.quantile(dg['age'], [0,.25,.5,.75,1])]\n",
    "print(f'Female {dg[\"Female\"].mean() * 100:.1f}%')\n",
    "print(f'Racial minority {dg[\"RacialMinority\"].mean() * 100:.1f}%')\n",
    "print(f'''Age: \n",
    "min \\t{ages[0]:}\n",
    "25th \\t{ages[1]}\n",
    "median \\t{ages[2]}\n",
    "75th \\t{ages[3]}\n",
    "max \\t{ages[4]}''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ac9798",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714d1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code for generating features.txt here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881074cc",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recreate Chart of comparison of different model MAEs\n",
    "\n",
    "\n",
    "data_file = \"master_frame5.csv\"\n",
    "features_file = \"features.txt\"\n",
    "target = \"NetDrop\"\n",
    "data_cleaning = False\n",
    "feature_selection = False\n",
    "pickle_file_name = \"current_model.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read the features file into a list\n",
    "features_list = []\n",
    "with open(features_file, 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        features_list.append(line.strip())\n",
    "    \n",
    "#Check to see the features list was read in properly\n",
    "if (len(features_list) == 0):\n",
    "    print(\"Something went wrong in reading the features file\")\n",
    "\n",
    "#Get the X,y columns to be used in training\n",
    "try:\n",
    "    X,y = CAPS.GetXy(X_columns = features_list, y_column = target, dropna = True, csv_file = data_file)\n",
    "except:\n",
    "    print(\"The csv file does not exist\")\n",
    "\n",
    "#Split X and y into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 2021)\n",
    "\n",
    "\n",
    "model_maes = {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e45591",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b04661",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = y_train.to_numpy()\n",
    "test_vals = y_test.to_numpy()\n",
    "average_drop = sum(y_vals)/len(y_vals)\n",
    "dif_from_average = [abs(x-average_drop) for x in test_vals]\n",
    "squared_dif = [(x-average_drop)**2 for x in test_vals]\n",
    "print(\"The mean squared error of the baseline model is:\", sum(squared_dif)/len(squared_dif))\n",
    "print(\"The mean absolute error of the baseline model is:\", sum(dif_from_average)/len(dif_from_average))\n",
    "model_maes['baseline'] = sum(dif_from_average)/len(dif_from_average)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a41c55",
   "metadata": {},
   "source": [
    "### Linear SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7674c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_config_dict = {\n",
    "    'sklearn.svm.LinearSVR': {\n",
    "        'loss': [\"epsilon_insensitive\", \"squared_epsilon_insensitive\"],\n",
    "        'dual': [True, False],\n",
    "        'tol': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1],\n",
    "        'C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.],\n",
    "        'epsilon': [1e-4, 1e-3, 1e-2, 1e-1, 1.]\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the Linear SVR...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of Linear SVR is: ', mse)\n",
    "print('The mean absolute error of Linear SVR is: ', mae)\n",
    "model_maes['linear svr'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f28503",
   "metadata": {},
   "source": [
    "### Lasso Lars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42e057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train TPOT on only Lasso Lars\n",
    "regressor_config_dict = {\n",
    "    'sklearn.linear_model.LassoLarsCV': {\n",
    "        'normalize': [True, False]\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the Lasso LARS...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of Lasso LARS is: ', mse)\n",
    "print('The mean absolute error of Lasso LARS is: ', mae)\n",
    "model_maes['lasso lars'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc8f207",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b3b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train TPOT on only XGBoost\n",
    "regressor_config_dict = {\n",
    "    'xgboost.XGBRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': range(1, 11),\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_child_weight': range(1, 21),\n",
    "        'n_jobs': [1],\n",
    "        'verbosity': [0],\n",
    "        'objective': ['reg:squarederror']\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the XGBoost...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of XGBoost is: ', mse)\n",
    "print('The mean absolute error of XGBoost is: ', mae)\n",
    "model_maes['xgboost'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30d934",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train TPOT on only Gradient Boosting\n",
    "regressor_config_dict = {\n",
    "    'sklearn.ensemble.GradientBoostingRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'loss': [\"ls\", \"lad\", \"huber\", \"quantile\"],\n",
    "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.],\n",
    "        'max_depth': range(1, 11),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'subsample': np.arange(0.05, 1.01, 0.05),\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'alpha': [0.75, 0.8, 0.85, 0.9, 0.95, 0.99]\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the Gradient Boosting model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of Gradient Boosting is: ', mse)\n",
    "print('The mean absolute error of Gradient Boosting is: ', mae)\n",
    "model_maes['gradient boosting'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f03d29",
   "metadata": {},
   "source": [
    "### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train TPOT on only K Nearest Neighbors\n",
    "regressor_config_dict = {\n",
    "    'sklearn.neighbors.KNeighborsRegressor': {\n",
    "        'n_neighbors': range(1, 101),\n",
    "        'weights': [\"uniform\", \"distance\"],\n",
    "        'p': [1, 2]\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the K Nearest Neighbors model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of K Nearest Neighbors is: ', mse)\n",
    "print('The mean absolute error of K Nearest Neighbors is: ', mae)\n",
    "model_maes['k neighbors'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbfaf68",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06135f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train TPOT on only Random Forest\n",
    "regressor_config_dict = {\n",
    "    'sklearn.ensemble.RandomForestRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the Random Forest model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of Random Forest is: ', mse)\n",
    "print('The mean absolute error of Random Forest is: ', mae)\n",
    "model_maes['random forest'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335b9439",
   "metadata": {},
   "source": [
    "### Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9dc957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train TPOT on only Extra Trees\n",
    "regressor_config_dict = {\n",
    "    'sklearn.ensemble.ExtraTreesRegressor': {\n",
    "        'n_estimators': [100],\n",
    "        'max_features': np.arange(0.05, 1.01, 0.05),\n",
    "        'min_samples_split': range(2, 21),\n",
    "        'min_samples_leaf': range(1, 21),\n",
    "        'bootstrap': [True, False]\n",
    "    },\n",
    "}\n",
    "\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1, config_dict=regressor_config_dict)\n",
    "print(\"Training the Extra Trees model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error of Extra Trees is: ', mse)\n",
    "print('The mean absolute error of Extra Trees is: ', mae)\n",
    "model_maes['extra trees'] = mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160c42d5",
   "metadata": {},
   "source": [
    "### Comparison Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3037a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.bar(model_maes.keys(), model_maes.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5bc1ed",
   "metadata": {},
   "source": [
    "### TPOT with all 30 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6833a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"master_frame5.csv\"\n",
    "features_file = \"features.txt\"\n",
    "target = \"NetDrop\"\n",
    "pickle_file_name = \"current_model.pkl\"\n",
    "\n",
    "\n",
    "\n",
    "#Read the features file into a list\n",
    "features_list = []\n",
    "with open(features_file, 'r') as myfile:\n",
    "    for line in myfile:\n",
    "        features_list.append(line.strip())\n",
    "\n",
    "#Check to see the features list was read in properly\n",
    "if (len(features_list) == 0):\n",
    "    print(\"Something went wrong in reading the features file\")\n",
    "    \n",
    "#Get the X,y columns to be used in training\n",
    "try:\n",
    "    X,y = CAPS.GetXy(X_columns = features_list, y_column = target, dropna = True, csv_file = data_file)\n",
    "except:\n",
    "    print(\"The csv file does not exist\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Split X and y into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2, random_state = 2021)\n",
    "\n",
    "#Checkpoint \n",
    "print(\"The size of the training data is:\", len(X_train))\n",
    "print(\"The size of the testing data is:\", len(y_test))\n",
    "\n",
    "#Have automl choose the model for us\n",
    "print(\"Starting the automl. This may take some time...\")\n",
    "model = TPOTRegressor(generations=3, population_size=50, scoring='neg_mean_absolute_error', verbosity=2, random_state=1, n_jobs=-1)\n",
    "print(\"The model has been chosen:\", model)\n",
    "print(\"Training the model...\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#Use the model to predict outcomes and report the accuracy of predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print('The mean squared error is: ', mse)\n",
    "print('The mean absolute error is: ', mae)\n",
    "\n",
    "\n",
    "#Save the model to a pickle file for ease of access in the future\n",
    "model.export('practice_export.py')\n",
    "print(model.fitted_pipeline_)\n",
    "pickle.dump(model.fitted_pipeline_, open(pickle_file_name, 'wb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c3340",
   "metadata": {},
   "source": [
    "### Top model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44352b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "files_y = (\"master_frame5.csv\", \"features.txt\", \"NetDrop\", \"current_model.pkl\")\n",
    "\n",
    "model, patientID, X,y = CAPS.GetModelPatientXy(input = files_y)\n",
    "\n",
    "r_2 = CAPS.METRIC(model=model, metric=r2_score, patientID=patientID, X=X, y=y)\n",
    "mse = CAPS.METRIC(model=model, metric=mean_squared_error, patientID=patientID, X=X, y=y)\n",
    "mae = CAPS.MAE(model=model, patientID=patientID, X=X, y=y)\n",
    "\n",
    "print(model.__str__())\n",
    "print(f'R-squared: {r_2:.4f}\\nMAE:       {mae:.2f}\\nRMSE:      {mse**.5:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44fb821",
   "metadata": {},
   "source": [
    "# Improvement Distribution\n",
    "Needs some updating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9afc353",
   "metadata": {},
   "source": [
    "### Calculating $\\sigma_\\epsilon^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f0109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bad estimator\n",
    "mse = CAPS.METRIC(model=model, metric=mean_squared_error, patientID=patientID, X=X, y=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8ad0ef",
   "metadata": {},
   "source": [
    "### Improvement.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def improvement(model, patientID, X, y, var_e, select_therapists = None, repetitions=5, n_splits=3, random_state=0, fun = max):\n",
    "    improvement = pd.DataFrame(columns=['y'], index=X.index)\n",
    "    improve_y = pd.DataFrame(columns=[f\"{r}\" for r in range(repetitions)], index=X.index)\n",
    "    improve_median = pd.DataFrame(columns=[f\"{r}\" for r in range(repetitions)], index=X.index)\n",
    "    therapists = [c for c in X.columns if 'TherapistID' in c]\n",
    "    patient_list = patientID.unique()\n",
    "\n",
    "    if select_therapists is None:\n",
    "        select_therapists = therapists\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    for ii in range(repetitions):\n",
    "        for train_p, test_p in kf.split(patient_list):\n",
    "            train_index = patientID[patientID.isin(patient_list[train_p])].index\n",
    "            test_index = patientID[patientID.isin(patient_list[test_p])].index\n",
    "\n",
    "            model.fit(X.loc[train_index, :], y[train_index].to_numpy().ravel())\n",
    "            outcomes = []\n",
    "            for therapist in select_therapists:\n",
    "                X_ = X.loc[test_index, :].copy()\n",
    "                X_[therapists] = 0\n",
    "                X_[therapist] = 1\n",
    "                outcomes.append(model.predict(X_))\n",
    "            optimal = pd.DataFrame(outcomes).apply(fun, axis=0)\n",
    "            improve_y.loc[test_index, ii] = (optimal - y[test_index].to_numpy().ravel()).values\n",
    "    improvement.loc[:, 'y'] = improve_y.mean(axis = 1) \n",
    "    improvement.loc[:, 'y'] += np.random.normal(scale = var_e**.5, size = (improve_y.shape[0]))\n",
    "    return improvement\n",
    "\n",
    "\n",
    "model, patientID, X,y = CAPS.GetModelPatientXy()\n",
    "imp_0 = improvement(model=model, patientID=patientID, X=X, y=y, \n",
    "                    var_e = 0, repetitions=2, random_state=0)\n",
    "imp_1 = improvement(model=model, patientID=patientID, X=X, y=y, \n",
    "                    var_e = mse, repetitions=2, random_state=0)\n",
    "rand_0 = improvement(model=model, patientID=patientID, X=X, y=y, \n",
    "                     var_e = 0, repetitions=2, random_state=0, fun = np.random.choice)\n",
    "rand_1 = improvement(model=model, patientID=patientID, X=X, y=y, \n",
    "                     var_e = mse, repetitions=2, random_state=0, fun = np.random.choice)\n",
    "#imp.to_csv(os.path.join(*data_path, 'improvement.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228a853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_0 = improvement(model=model, patientID=patientID, X=X, y=y, var_e = 0, repetitions=2, random_state=0, fun = np.random.choice)\n",
    "rand_1 = improvement(model=model, patientID=patientID, X=X, y=y, var_e = mse, repetitions=2, random_state=0, fun = np.random.choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358e40ee",
   "metadata": {},
   "source": [
    "### $\\mathbb{E}$[ $I$ ], $P(I > 0)$, and $P(I > 14)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c7371e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_i = imp_0['y'].mean()\n",
    "mean_r = rand_0['y'].mean()\n",
    "p_0 = [np.mean(imp_1['y'] > 0), np.mean(imp_0['y'] > 0), np.mean(rand_1['y'] > 0), np.mean(rand_0['y'] > 0)]\n",
    "p_14 = [np.mean(imp_0['y'] > 14), np.mean(imp_1['y'] > 14), np.mean(rand_0['y'] > 14), np.mean(rand_1['y'] > 14)]\n",
    "\n",
    "print(f\"\"\"Estimates\n",
    "E(I) = {mean_i:.2f}\n",
    "P(I > 0) in ({p_0[0]:.3f}, {p_0[1]:.3f})\n",
    "P(I > 14) in ({p_14[0]:.3f}, {p_14[1]:.3f})\n",
    "\n",
    "Random therapist\n",
    "E(I) = {mean_r:.2f}\n",
    "P(I > 0) in ({p_0[2]:.3f}, {p_0[3]:.3f})\n",
    "P(I > 14) in ({p_14[2]:.3f}, {p_14[3]:.3f})\n",
    "\"\"\")\n",
    "\n",
    "print(f\"sd {imp_0['y'].std():.0f}, with error added {imp_1['y'].std():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73f477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27701dce",
   "metadata": {},
   "source": [
    "### Bootstrapped numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea008fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def bootstrap(function, model, X, y, repetitions=1000):\n",
    "    therapists = [c for c in X.columns if 'TherapistID' in c]\n",
    "\n",
    "    out = []\n",
    "    for _ in range(repetitions):\n",
    "        train_i = choices(sorted(y.index), k = len(y))\n",
    "        test_i = list(set(y.index) - set(train_i))\n",
    "\n",
    "        model.fit(X.loc[train_i], y.loc[train_i].to_numpy().ravel())\n",
    "\n",
    "        outcomes = []\n",
    "        for therapist in therapists:\n",
    "            X_ = X.loc[test_i].copy()\n",
    "            X_[therapists] = 0\n",
    "            X_[therapist] = 1\n",
    "            outcomes.append(model.predict(X_))\n",
    "        optimal = pd.DataFrame(outcomes, index=therapists).apply(np.max, axis=0)\n",
    "        improve = optimal - y.loc[test_i].to_numpy().ravel()\n",
    "        out.append(function(improve))\n",
    "    upper = sorted(out)[round(repetitions * .975) -1]\n",
    "    lower = sorted(out)[round(repetitions * .25) - 1]\n",
    "    return {'Mean': np.mean(out), '95% CI': (lower, upper), 'Values': out}\n",
    "\n",
    "print('Improvement greater than 14 point increase:')\n",
    "fourteen = bootstrap(lambda x: np.mean(x >= 14), trees, X, y)\n",
    "print(\"{Mean} {95% CI}\".format(**fourteen))\n",
    "# \n",
    "\n",
    "print('Non negative improvement')\n",
    "nonnegative = bootstrap(lambda x: np.mean(x >= 0), trees, X, y)\n",
    "print(\"{Mean} {95% CI}\".format(**nonnegative))\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f75491d",
   "metadata": {},
   "source": [
    "### Assesing bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76b2fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#imp = pd.read_csv(os.path.join(*data_path, \"improvement.csv\"))\n",
    "imp = imp_1\n",
    "_, X_m, _ = CAPS.GetPatientXy(y_column='NetDrop',  dropna=True)\n",
    "imp.index = [ i for i in range(imp.shape[0]) ]\n",
    "X_m.index = [ i for i in range(X_m.shape[0]) ]\n",
    "\n",
    "def get_4groups(series):\n",
    "    q = np.percentile(series, [25, 50, 75])\n",
    "    groups = pd.DataFrame(columns=['g'], index=series.index)\n",
    "    groups.loc[(series <= q[0]), 'g'] = 0\n",
    "    groups.loc[(series >= q[0]) & (series <= q[1]), 'g'] = 1\n",
    "    groups.loc[(series >= q[1]) & (series <= q[2]), 'g'] = 2\n",
    "    groups.loc[(series >= q[2]), 'g'] = 3\n",
    "    return groups\n",
    "\n",
    "\n",
    "B = pd.concat([X_m.drop([c for c in X_m.columns if 'TherapistID' in c], axis=1), get_4groups(imp['y'])], axis=1)\n",
    "B['Disability'] = (B['Disabilities'] > 0).astype(int)\n",
    "\n",
    "B_columns = pd.DataFrame(B.columns, columns=['o'], index=B.columns)\n",
    "\n",
    "for character in [\",\", \"(\", \")\", \" \", \"'\", \"-\", \"/\", \"&\",\".\", \"?\"]:\n",
    "    B_columns.index = [c.replace(character, '') for c in B_columns.index]\n",
    "\n",
    "B.columns = B_columns.index\n",
    "\n",
    "mean_table = pd.DataFrame(columns=B.drop(['g'], axis=1).columns)\n",
    "for level, data in B.groupby(\"g\"):\n",
    "    mean_table.loc[int(level), 'n'] = data.shape[0]\n",
    "    for c in mean_table.drop(\"n\", axis=1).columns:\n",
    "        mean_table.loc[int(level), c] =  data[c].mean()\n",
    "\n",
    "list_c = [\"Female\",\n",
    "          \"RacialMinority\",\n",
    "          \"age\",\n",
    "          \"InternationalStudent\",\n",
    "          \"SexOrientationMinority\",\n",
    "          \"Disability\",\n",
    "          \"ReligiousMinority\",\n",
    "          \"AgnosticOrAtheist\"]\n",
    "\n",
    "anova_table = pd.DataFrame(columns=['c', 'p'])\n",
    "for i, c in enumerate(list_c):\n",
    "    model_anova = smf.ols(f'{c} ~ C(g)', data=B).fit()\n",
    "    try:\n",
    "        aov_table = sm.stats.anova_lm(model_anova, typ=2)\n",
    "        anova_table.loc[i, 'c'] = B_columns.loc[c, 'o']\n",
    "        anova_table.loc[i, 'p'] = aov_table.iloc[0]['PR(>F)']\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "anova_table.sort_values(\"p\", inplace=True)\n",
    "\n",
    "\n",
    "# Benjamini–Yekutieli procedure\n",
    "def ben_yek(p_sorted):\n",
    "    p_sorted\n",
    "    m = len(p_sorted)\n",
    "    c = np.log(m) + .57721 + (1 / (2*m))\n",
    "    significant = [ p_k <= (k+1) / (m * c) * 0.05  for k, p_k in enumerate(p_sorted)]\n",
    "    return significant\n",
    "\n",
    "anova_table[\"significant\"] = ben_yek(anova_table['p'])\n",
    "print(anova_table.loc[anova_table[\"significant\"], :])\n",
    "\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 18}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "for cc in anova_table.loc[anova_table[\"significant\"], \"c\"]:\n",
    "    c = B_columns.loc[B_columns['o'] == cc, :].index[0]\n",
    "    sns.catplot(data=B,\n",
    "                x='g',\n",
    "                hue=cc, \n",
    "                kind=\"count\",\n",
    "               palette='Blues')\n",
    "    plt.xlabel('Quartiles of Improvement')\n",
    "    plt.xticks([0, 3], labels=['lowest', 'highest'])\n",
    "    plt.ylabel(cc)\n",
    "    plt.savefig(f\"../figures/mean-{c}.png\")\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d06cdb",
   "metadata": {},
   "source": [
    "### Therapist Effect and number of clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def print_counter_fact(X, improve, counter_fact, comment, column, mod_func) -> None:\n",
    "  improve = pd.concat([improve, counter_fact], axis=1)\n",
    "  improve.columns = [\"y\", \"y_m\"]\n",
    "  print(comment)\n",
    "  print(col)\n",
    "  temp = pd.DataFrame(columns=['level', 'improve_hat', 'sd', 'improvement'])\n",
    "  for i, l in enumerate(X[column].unique()):\n",
    "      temp.loc[i, 'level'] = l\n",
    "      temp.loc[i, 'improve_hat'] = improve[X[col]==l][['y_m', \"y\"]].apply(lambda x: x[0]-x[1], axis=1).mean()\n",
    "      temp.loc[i, 'sd'] = improve[X[col]==l][['y_m', \"y\"]].apply(lambda x: x[0]-x[1], axis=1).std()\n",
    "      temp.loc[i, 'improvement'] = mod_func(l)\n",
    "  print(temp.sort_values('level'))\n",
    "\n",
    "col = 'I feel sad all the time'\n",
    "y_modified = pd.concat([y, X[['TherapistID_451', col]]], axis=1).apply(\n",
    "    lambda x: x[0] + x[1] * x[2] * 14 / 4, axis=1).to_frame()\n",
    "# This therapist appears the most in this data set\n",
    "\n",
    "imp_1 = [improvement(model=trees, X=X, y=y_modified, select_therapists=['TherapistID_451']),\n",
    "          improvement(model=trees, X=X, y=y, select_therapists=['TherapistID_451'])]\n",
    "\n",
    "print_counter_fact(X=X, improve=imp_1[1], counter_fact=imp_1[0],\n",
    "                    comment='Therapist with the most sessions',\n",
    "                    column=col, mod_func=lambda x: x * 14 / 4)\n",
    "'''\n",
    "  level improve_hat        sd improvement\n",
    "4   0.0    2.117247  1.870019         0.0\n",
    "2   1.0    3.249249  2.343237         3.5\n",
    "1   2.0    6.395659  4.207738         7.0\n",
    "0   3.0    8.182834  5.137098        10.5\n",
    "3   4.0   12.140104  5.382635        14.0\n",
    "'''\n",
    "\n",
    "col = 'I feel disconnected from myself'\n",
    "print(col)\n",
    "y_modified = pd.concat([y, X[['TherapistID_451', col]]], axis=1).apply(\n",
    "    lambda x: x[0] + x[1] * x[2] * 14 / 4, axis=1).to_frame()\n",
    "\n",
    "imp_2 = [improvement(model=trees, X=X, y=y_modified, select_therapists=['TherapistID_451']),\n",
    "          improvement(model=trees, X=X, y=y, select_therapists=['TherapistID_451'])]\n",
    "\n",
    "print_counter_fact(X=X, improve=imp_2[1], counter_fact=imp_2[0],\n",
    "                    comment='Therapist with the most sessions',\n",
    "                    column=col, mod_func=lambda x: x * 14 / 4)\n",
    "'''\n",
    "level improve_hat        sd improvement\n",
    "2   0.0    2.056419  3.331124         0.0\n",
    "3   1.0    5.247705  3.480443         3.5\n",
    "1   2.0    8.307339  4.037828         7.0\n",
    "4   3.0    9.411611  4.721901        10.5\n",
    "0   4.0   11.799676  5.652768        14.0\n",
    "'''\n",
    "\n",
    "print('18 therapist with the fewest')\n",
    "col = 'I feel sad all the time'\n",
    "low_app_therapists = [f\"TherapistID_{id}\" for id in [1053, 969, 1023, 1050, 943, 917, 1052, 913, 1070, 972, 1002,\n",
    "            968, 1025, 1043, 225, 923, 187]] # These therapist have the fewest appointments\n",
    "            # 43, 1057 were in this list but then excluded because it was giving me errors\n",
    "summed_therap = X[low_app_therapists].sum(axis=1).to_frame()\n",
    "summed_therap.columns = ['Therapists']\n",
    "\n",
    "y_modified = pd.concat([y, \n",
    "                        pd.concat([summed_therap, X[col]], axis=1)],\n",
    "                        axis=1).apply(\n",
    "    lambda x: x[0] + x[1] * x[2] * 14 / 4, axis=1).to_frame()\n",
    "\n",
    "imp_3 = [improvement(model=trees, X=X, y=y_modified, select_therapists=low_app_therapists),\n",
    "          improvement(model=trees, X=X, y=y, select_therapists=low_app_therapists)]\n",
    "\n",
    "print_counter_fact(X=X, improve=imp_3[1], counter_fact=imp_3[0],\n",
    "                    comment='Therapist with the most sessions',\n",
    "                    column=col, mod_func=lambda x: x * 14 / 4)\n",
    "'''\n",
    "  level improve_hat        sd improvement\n",
    "4   0.0    0.724389  0.801883         0.0\n",
    "2   1.0    0.989333  1.049811         3.5\n",
    "1   2.0    3.373035  1.855928         7.0\n",
    "0   3.0    3.760619  2.355769        10.5\n",
    "3   4.0    3.536887  2.734238        14.0\n",
    "'''\n",
    "\n",
    "col = 'I feel disconnected from myself'\n",
    "y_modified = pd.concat([y, \n",
    "                        pd.concat([summed_therap, X[col]], axis=1)],\n",
    "                        axis=1).apply(\n",
    "    lambda x: x[0] + x[1] * x[2] * 14 / 4, axis=1).to_frame()\n",
    "\n",
    "imp_4 = [improvement(model=trees, X=X, y=y_modified, select_therapists=low_app_therapists),\n",
    "          improvement(model=trees, X=X, y=y, select_therapists=low_app_therapists)]\n",
    "\n",
    "print_counter_fact(X=X, improve=imp_4[1], counter_fact=imp_4[0],\n",
    "                    comment='Therapist with the most sessions',\n",
    "                    column=col, mod_func=lambda x: x * 14 / 4)\n",
    "'''\n",
    "  level improve_hat        sd improvement\n",
    "2   0.0    2.332845  1.710144         0.0\n",
    "3   1.0    2.772999  1.836964         3.5\n",
    "1   2.0    3.488588  2.012908         7.0\n",
    "4   3.0    5.161405  2.524587        10.5\n",
    "0   4.0    5.165624  3.078474        14.0\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7431774b",
   "metadata": {},
   "source": [
    "### Unequal distribution of chosen therapists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab9b2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code for the chosen one graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f38a1ee",
   "metadata": {},
   "source": [
    "# Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52506933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code for the batching problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7afaa7",
   "metadata": {},
   "source": [
    "# Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1166d238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert code for the streaming problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c0d30e2784ab47031ef102b63182cf7ef6f008236a4112f0340c4af3368b1081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
